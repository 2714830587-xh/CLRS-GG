{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoftMax  and   CrossEntropyLoss.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMD/XFDuVnqTiEBhnqebGab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2714830587-xh/CLRS-GG/blob/master/SoftMax_and_CrossEntropyLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **softmax**\n",
        "numpy 和tensor 版本"
      ],
      "metadata": {
        "id": "yk0Sn3PfbVoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m4HCLazZvq9",
        "outputId": "1be8901e-6355-4da4-fd20-fe158eed4481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax_numpy [0.65900114 0.24243297 0.09856589]\n",
            "softmax_tensor tensor([0.6590, 0.2424, 0.0986])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "def softmax(x):\n",
        "  return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
        "\n",
        "x=np.array([2.0,1.0,0.1])\n",
        "output=softmax(x)\n",
        "print(\"softmax_numpy\",output)\n",
        "\n",
        "x=torch.tensor([2.0,1.0,0.1])\n",
        "output=torch.softmax(x,dim=0)\n",
        "\n",
        "print(\"softmax_tensor\",output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **cross_entropy**\n",
        "numpy 和tensor 版本\n",
        "\n",
        "tensor内置的有以下不同:\n",
        "\n",
        "1.运用了nn.LogSoftmax+nn.NLLLoss,所以不需要softmax\n",
        "\n",
        "2.Y has class labels,not One-hot,and y_pre have raw socores,no softmaax\n"
      ],
      "metadata": {
        "id": "n-bXD4gwdaIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "def cross_entropy(actual,predicted):\n",
        "  loss=-np.sum(actual*np.log(predicted))\n",
        "  return loss\n",
        "\n",
        "Y=np.array([1,0,0])\n",
        "Y_pre_good=np.array([0.7,0.2,0.1])\n",
        "Y_pre_bad=np.array([0.1,0.3,0.6])\n",
        "l1=cross_entropy(Y,Y_pre_good)\n",
        "l2=cross_entropy(Y,Y_pre_bad)\n",
        "print(\"好的预测的损失\",l1)\n",
        "print(\"坏的预测的损失\",l2)\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#3 sample 注意这是标签而不是one-hot\n",
        "# Y=torch.tensor([2,0,1])\n",
        "Y=torch.tensor([2,])\n",
        "#pre num_sample*num_class  1*3 or 3*3\n",
        "Y_Pre_good=torch.tensor([[0.1,1.0,2.1]])\n",
        "Y_Pre_bad=torch.tensor([[2.1,1.0,0.1]])\n",
        "# Y_Pre_good=torch.tensor([[0.1,1.0,2.1],[2.0,1.0,0.1],[0.1,3.0,0.1]])\n",
        "# Y_Pre_bad=torch.tensor([[2.1,1.0,0.1],[0.1,1.0,2.1],[0.1,3.0,0.1]])\n",
        "l1=loss(Y_Pre_good,Y)\n",
        "l2=loss(Y_Pre_bad,Y)\n",
        "print(\"好的预测的损失\",l1)\n",
        "print(\"坏的预测的损失\",l2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWK3WOWrblpv",
        "outputId": "83b741ef-332f-4c14-d01b-44290656e51a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "好的预测的损失 0.35667494393873245\n",
            "坏的预测的损失 2.3025850929940455\n",
            "好的预测的损失 tensor(0.3840)\n",
            "坏的预测的损失 tensor(2.3840)\n"
          ]
        }
      ]
    }
  ]
}